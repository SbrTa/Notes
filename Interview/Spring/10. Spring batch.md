# Spring batch


#### ✅ What is Spring Batch?
Spring Batch is a lightweight, comprehensive framework designed to facilitate the development of robust and scalable **batch processing applications** in the Java programming language. It is part of the larger Spring Framework ecosystem and provides a set of reusable components for batch processing.


#### ✅ Important features of Spring Batch.
Here are some important points highlighting why one might choose to use Spring Batch:
  - **Transaction Management**: Ensures atomicity in batch processing by managing transactions. Allows for rolling back changes in case of errors to maintain data integrity.
  - **Chunk Based Processing**: Efficiently processes large datasets by breaking them into smaller, manageable chunks. Enables parallelization for improved performance.
  - **Declarative I/O**: Abstracts I/O operations, providing high-level components (ItemReader, ItemProcessor, ItemWriter) for reading, processing, and writing data.  It provides I/O support for JMS, Database, File System and more.
  - **Start/Stop/Restart**: Provides flexibility in managing batch jobs by supporting initiation, halting, and resuming of job execution. Allows for easy restart of jobs in case of failures.
  - **Retry/Skip**: Handles errors during processing by offering mechanisms for retrying or skipping items. 
  - **Web based administration interface**: Offers a user-friendly web interface for monitoring and managing batch jobs. Provides insights into job and step statuses, execution history, and parameter management.


#### ✅ Explain the Spring Batch framework architecture?
A batch process is typically encapsulated by a **Job** consisting of multiple **Steps**. Each **Step** typically has a single **ItemReader**, **ItemProcessor**, and **ItemWriter**. A **Job** is executed by a **JobLauncher**, and metadata about configured and executed jobs is stored in a **JobRepository**.

  - **JobLauncher**: The JobLauncher is responsible for initiating the execution of a job. It takes the job configuration and triggers the processing of steps within the job.
  - **JobRepository**: The JobRepository stores metadata about jobs and steps. It is responsible for managing the state of job executions, including information about completed, running, or failed jobs. The JobRepository facilitates features like restartability.
  - **Job**: Represents a complete and independent unit of work. It is composed of one or more steps, defining the overall processing logic.
  - **Step**: A step is a single, independent unit of work within a job. It consists of an ItemReader, an optional ItemProcessor, and an ItemWriter. Each step processes a chunk of data in a transactional manner.
    - **ItemReader**: An ItemReader is responsible for reading data from a specified source and providing it to the ItemProcessor. It reads data in chunks, and its role is to feed the data to the subsequent processing steps.
    - **ItemProcessor**: An ItemProcessor is optional and allows for the transformation or processing of the data read by the ItemReader before it is passed to the ItemWriter. It operates on each item within a chunk.
    - **ItemWriter**: An ItemWriter is responsible for writing the processed data to a specified destination. It receives data from the ItemProcessor and persists or outputs it to the desired target, such as a database or file.
  
![image](https://github.com/SbrTa/Notes/assets/8649145/73f0a6bf-3e01-4a27-bca5-f1bf1a78c397)

#### ✅ Tasklet vs Chunk
In Spring Batch, both Tasklet and Chunk are approaches for defining the processing logic within a Step.
  - **Tasklet**: A Tasklet is an interface in Spring Batch that represents a single task or operation within a Step. It encapsulates the entire processing logic of a step. Tasklets are suitable when the processing logic is relatively simple and can be executed within a single transaction. A Tasklet can be used to perform a specific task, such as moving files, sending emails, or executing a custom operation.
  - **Chunk**: The chunk-oriented processing model in Spring Batch breaks down the processing of data into chunks. A chunk consists of reading a specified number of items, processing them, and then writing the results. Chunks are suitable for scenarios where processing large datasets efficiently is a priority. This approach allows for parallelization and transactional processing of data.



#### ✅ 
#### ✅ 
#### ✅ 
#### ✅ 
#### ✅ 
#### ✅ 
