# Topics, Partitions and Offset
  - Kafka topics are a particular stream of data within your Kafka cluster.
  - Kafka cluster can have many topics.
  - A topic is identified by its name.
  - So, a topic in Kafka is a stream of data.
  - Kafka is called a data streaming platform, because you make data stream through topics.
  - A topic is similar to what a table would be in a database, but without all the constraints. Because you send whatever you want to a Kafka topic, there is no data verification.
  - you can send, for example, Json, Avro, text file, binary, whatever you want.
  - So topics are similar to table database, but you cannot query them, instead to add data into a Kafka topic, And read data from a topic.
  - you can divide topics into partitions. a topic can be made up of for example, 100 partitions.
  - Now the messages sent to Kafka topic are going to end up in these partitions,
  - messages within each partition are going to be ordered.
  - So, the messages in these partitions where they are written, they are getting a id, that's incrementing from zero to whatever. And this id is called a Kafka partition offset.
  - Kafka topics are also immutable. That means that once the data is written into a partition, it cannot be changed.
  - Data in Kafka is only kept for a limited time. And the default is one week, although that is configurable. That means that after one week, your data will disappear.
  - And the offsets only have a meaning for a specific partition. the offsets are repeated across partitions.
  - the data when sent to a Kafka topic, is going to be assigned to a random partition, Unless you provide a message key,

# Producer and Message key
  - Producers write data to Kafka topics and partitions.
  - Topics have partitions, and writes happen sequentially with data offsets.
  - Producers send data to Kafka topic partitions.
  - Producers know in advance to which partition they write.
  - Producers send data across all partitions using load balancing.
  - Kafka scales due to multiple partitions within a topic.
  - Producers assign keys to messages for ordering and identification.
  - Messages with the same key go to the same partition.
  - Producers decide where messages end up based on the key.
  - Kafka messages are created using serializers to transform objects into bytes.
  - Key and value serializers can be different.
  - Kafka partitioners determine which partition a message goes to.
  - Key hashing uses algorithms like murmur2 for mapping keys to partitions.

![image](https://github.com/SbrTa/Notes/assets/8649145/e513078f-ef87-4185-a792-9f697537dae0)


# Consumer and Deserialization
- Consumers are used to read data from Kafka topics.
- They operate on a pull model, requesting data from Kafka brokers.
- Consumers can choose the partitions they read from.
- An example with three partitions demonstrates different consumers reading from specific partitions.
- Consumers automatically handle broker failures and know how to recover.
- Data is read in order within each partition, from low to high offset.
- Consumers need to transform bytes received from Kafka into usable objects.
- Deserializers are used to convert binary key and value formats into appropriate objects.
- Consumers must know the format of messages in advance.
- Deserializers, bundled with Kafka, handle the transformation based on expected formats (e.g., integer, string).
- Changing the data type of a topic after creation can break consumers.
- Creating a new topic with the desired format is recommended for changes, requiring consumer adjustments.
- Consumers play a vital role in reading and processing Kafka messages.
- Understanding serialization and data format changes is crucial for effective consumer usage.

# Consumers Groups & consumer offset
- Kafka applications scale by having multiple consumers in a group.
- Consumers within a group read data collectively from Kafka topics.
- Each consumer reads from exclusive partitions, ensuring efficient distribution.
- If there are more consumers than partitions, additional consumers may remain inactive.
- Inactive consumers won't read from any partitions.
- It's acceptable to have multiple consumer groups on the same topic.
- Each partition may have multiple readers, but within a consumer group, only one consumer is assigned to one partition.
- Distinct consumer groups are created using the `group.id` consumer property.
- Multiple consumer groups are useful for different services, like a location service and a notification service.
- Kafka stores consumer offsets in an internal topic named "consumers_offsets."
- Committing offsets is crucial for tracking the progress of a consumer group.
- Consumer offsets allow replaying data in case of consumer failures or restarts.
- Consumers can commit offsets either automatically (at least once by default) or manually.
- Three delivery semantics: at least once, at most once, and exactly once.
- Considerations for idempotent processing to avoid duplicate impacts.


# Broker & Topics

A Kafka cluster is an ensemble of multiple Kafka brokers. Unlike typical servers, Kafka brokers are specialized for receiving and sending data.

Each Kafka broker is identified by an integer ID, such as Broker 101, Broker 102, and Broker 103 in our cluster.


Brokers contain specific topic partitions, leading to distributed data across all brokers in the cluster. This distribution ensures scalability, and Kafka can handle clusters with varying numbers of brokers, ranging from a few to over 100.

In our example, we've discussed numbering brokers starting at 100 for simplicity.


Let's consider an example with Topic-A having three partitions and Topic-B having two partitions. We have three brokers: Broker 101, Broker 102, and Broker 103.

- Broker 101 has Topic-A, Partition 0.
- Broker 102 has Topic-A, Partition 2.
- Broker 103 has Topic-A, Partition 1.

For Topic-B:
- Broker 101 has Topic-B, Partition 1.
- Broker 102 has Topic-B, Partition 0.

This distribution illustrates Kafka's horizontal scaling, where data and partitions are spread across the entire cluster.


Kafka uses a bootstrap server mechanism for broker discovery. When connecting to any Kafka broker (bootstrap broker), clients automatically learn how to connect to the entire cluster. This smart mechanism simplifies cluster connection for clients.

# Topic Replication

When you're working on your own machine, topics can have a replication factor of one. However, in production, especially with a real Kafka cluster, the replication factor is usually set greater than one, commonly between two and three. This ensures that if a broker is down for maintenance or technical issues, other brokers still have copies of the data to serve and receive.

Let's take an example to understand this better. We have Topic-A with two partitions and a replication factor of two. Three Kafka brokers are in play.
- Partition 0 of Topic-A is on broker 101.
- Partition 1 of Topic-A is on broker 102.

Due to the replication factor, we have copies of partition 0 on broker 102 and partition 1 on broker 103. In case we lose broker 102, broker 101 and 103 can still serve the data, ensuring availability.

Replicas and Leaders:
- Kafka operates with a leader for each partition.
- Producers can only send data to the leader broker of a partition.
- Consumers read data primarily from the leader of a partition.

This design ensures a reliable and efficient data flow within Kafka clusters. 

Additionally, Kafka Consumer Replica Fetching, introduced in Kafka 2.4, allows consumers to read from the closest replica, improving latency and potentially reducing network costs.


# Producer Acknowledgement and Topic Durability

So we have the producers sending data into our brokers, okay? Now, we know that brokers have the topic partitions. And so the producers can choose to receive acknowledgements of data rights. That means to have the confirmation from the Kafka broker that the write did successfully happen.

We have three settings, and we have:
- acks=0, which means that the producer is not even going to wait or ask for an acknowledgement. That means that we have a possible data loss because if a broker goes down, we won't know about it.
- acks=1, which means that the producer is going to wait for the leader of a partition. And this is very important, that we understand now what a leader is. So, it's going to wait for the leader broker to acknowledge. And that means we have limited data loss, and I will go over this in details later on this course, but just introduction.
- acks=all, in which we require the leader, as well as all replicas, in sync replicas, to acknowledge the write which is going to provide you a guarantee of no data loss under certain circumstances. Okay?
  - min.insync.replicas=N : Leader and N-1 replicas need to ack.

So that means also that we can now understand the concept of Kafka topic durability. So, if you have a Kafka topic with a replication factor of three then the topic can withstand two brokers loss. In this example, I didn't take three brokers, I took two brokers, okay? Three brokers in a replication factor of two. And we saw that when we lost broker 102 we still had the topic data available to us because it was on other brokers, okay?

As a general rule, if you choose a replication factor of N, N being a number, then you can permanently lose up to N-1 broker and still have a copy of your data somewhere in your cluster.



# Zookeeper

Zookeeper has been how Kafka was able to function all the way up until today, but it's slowly disappearing and it's going to be replaced.

First, let's look at Kafka with Zookeeper. Zookeeper is managing Kafka brokers, and Zookeeper is a software. Zookeeper is going to keep a list of your Kafka brokers. Zookeeper is also going to be very helpful for Kafka because whenever we have a broker going down, we need to perform a leader election to choose a new leader for partitions, and Zookeeper is going to help with this process. Also, Zookeeper is going to send notifications to Kafka brokers in case of changes, such as when a new topic is created, when a Kafka broker goes down or comes up, deletion of topics, and so on. So, Zookeeper has a lot of the Kafka metadata.

Kafka, up until version 2.x, 2.8, cannot work without Zookeeper. So, Zookeeper has been, since the beginning of Kafka, a companion to Kafka brokers, and you cannot launch Kafka without launching Zookeeper. But now, starting with Kafka 3.x, you can have Kafka work on its own without Zookeeper; it's called the Kafka Raft mechanism. If you want to read more about it, just go on Google and type KIP (Kafka Improvement Proposal) 500. Then, in version Kafka 4.x, you will not have Zookeeper anymore. Right now, the community is transitioning to making Kafka work with Zookeeper correctly and then, at some point, migrate to a Zookeeper-less Kafka.

So, Kafka with Zookeeper is necessary right now, and you still need to learn about Zookeeper because you are going to see it in production a lot. Zookeeper, by design, is going to operate with an odd number of servers. So, you either have one Zookeeper or three Zookeeper, or five Zookeeper, or seven Zookeeper - never more than seven usually. Zookeeper also has a concept of leaders and the rest are followers. One for writes and the rest for reads.

Something you may read on the internet, but it's old information, and I still see it, is information for you to know: Kafka consumers in the old versions of Kafka used to store consumer offsets on Zookeeper. But now, as we know, they store consumer offsets on the internal Kafka topics named 'consumer_offsets'. So Zookeeper does not hold any consumer data, starting with Kafka version 0.10.

So the question is, should you use Zookeeper? If you are managing Kafka brokers, the answer is yes. Until Kafka 4.0 is out and ready, then you should not use Kafka without Zookeeper in production. But I'm still going to show you in the hands-on how to start Kafka without Zookeeper for you to have a play. But remember, it is not production-ready yet.

Kafka clients over time have been migrated to leverage the brokers as the only connection endpoint instead of Zookeeper. Before, you used to connect your producer to Zookeeper, your consumer to Zookeeper, your administration client to Zookeeper, and so on. You may see the Zookeeper option on online literature, but if you're doing the most recent version of this course, then you should not use Zookeeper anymore. All the Kafka clients and CLI tools have been migrated to only leverage Kafka brokers as connection endpoints. Even consumers that shouldn't connect to Kafka brokers because to Zookeepers, excuse me. And since Kafka 2.0, 2.2, the Kafka topics command also references Kafka brokers and not Zookeeper. This is very important because the community did an effort to migrate all the comments before from Zookeeper to Kafka. Because when we are going to have Kafka without Zookeeper, then the clients will not have any issues because they don't expect Zookeeper to be here.

Also, the reason why Zookeeper is going away is because Zookeeper is less secure than Kafka. So, you should protect Zookeeper if you use it to only accept connections from Kafka brokers but not from Kafka clients.


# Kraft

Apache Kafka® Raft (KRaft) is the consensus protocol that was introduced to remove Kafka's dependency on ZooKeeper for metadata management. KRaft eliminates the Zookeeper dependency, boosting scalability, stability, and administration simplicity. Production-ready since Kafka 3.3.1, KRaft will be the sole support in Kafka 4.0.
The KRaft architecture streamlines by removing the Zookeeper Quorum, improving control shutdown and recovery times. 


# Rebalancing

Consumer groups handle partitions when consumers join or leave, known as rebalancing. The default strategy is eager rebalance, causing a temporary stop-the-world event, disrupting processing. This may lead to partitions being randomly reassigned.

An alternative is the cooperative rebalance, introduced in Kafka 3.0, minimizing disruption. It incrementally reassigns small subsets of partitions, allowing unaffected consumers to continue processing. The CooperativeStickyAssignor is recommended for stability, balancing, and avoiding stop-the-world events.

To enable cooperative rebalance, use the CooperativeStickyAssignor, replacing eager strategies. Kafka Connect and Kafka Streams have cooperative rebalance enabled by default.

Static group membership, facilitated by a group instance ID in consumer config, allows consumers to be static members. When a static member leaves and rejoins within the session time, partitions aren't reassigned, preventing unnecessary rebalances. This is beneficial for scenarios like maintaining local caches.


# Offset

![image](https://github.com/SbrTa/Notes/assets/8649145/9479b12f-ed4e-420a-bb6c-eb301a43ea5b)


# Safe kafka producer settings

Kafka 3.0 Defaults (Safe by Default):
- acks: all (minuses one)
- enable.idempotence: true

Kafka 2.8 and Lower Defaults:
- acks: 1
- enable.idempotence: false

Recommendations:
- Always use a safe producer for data integrity.
- Upgrade Kafka clients to at least 3.0 for safety by default.
- If using Kafka 2.8 or lower, manually set:
  - acks: all
  - min.insync.replicas: 2 (ensure replication factor of at least 3)
  - enable.idempotence: true
  - retries: MAX_INT (to ensure retries until the delivery timeout)
  - delivery.timeout.ms: 120000 (two minutes)
  - max.in.flight.requests.per.connection: 5 (for performance while maintaining ordering)

Final Note:
- Kafka 3.0 makes it super easy with safe defaults.
- For versions lower than 3.0, manually configure the mentioned settings.


# Idempotent producer

**Problem with Retries:**

Retrying to send a failed message often includes a small risk that both messages were successfully written to the broker, leading to duplicates. This can happen as illustrated below.
- Kafka producer sends a message to Kafka
- The message was successfully written and replicated
- Network issues prevented the broker acknowledgment from reaching the producer
- The producer will treat the lack of acknowledgment as a temporary network issue and will retry sending the message (since it can’t know that it was received).
- In that case, the broker will end up having the same message twice.

![image](https://github.com/SbrTa/Notes/assets/8649145/289df6f8-5692-4568-933d-f66f679f9b9f)


An idempotent producer in Apache Kafka is a producer configuration setting that ensures that messages sent by the producer are delivered to the Kafka broker in an idempotent manner. Producer idempotence ensures that duplicates are not introduced due to unexpected retries.

![image](https://github.com/SbrTa/Notes/assets/8649145/b6bc4dfa-c148-4dd5-b498-e4e322e98882)
