# Topics, Partitions and Offset
  - Kafka topics are a particular stream of data within your Kafka cluster.
  - Kafka cluster can have many topics.
  - A topic is identified by its name.
  - So, a topic in Kafka is a stream of data.
  - Kafka is called a data streaming platform, because you make data stream through topics.
  - A topic is similar to what a table would be in a database, but without all the constraints. Because you send whatever you want to a Kafka topic, there is no data verification.
  - you can send, for example, Json, Avro, text file, binary, whatever you want.
  - So topics are similar to table database, but you cannot query them, instead to add data into a Kafka topic, And read data from a topic.
  - you can divide topics into partitions. a topic can be made up of for example, 100 partitions.
  - Now the messages sent to Kafka topic are going to end up in these partitions,
  - messages within each partition are going to be ordered.
  - So, the messages in these partitions where they are written, they are getting a id, that's incrementing from zero to whatever. And this id is called a Kafka partition offset.
  - Kafka topics are also immutable. That means that once the data is written into a partition, it cannot be changed.
  - Data in Kafka is only kept for a limited time. And the default is one week, although that is configurable. That means that after one week, your data will disappear.
  - And the offsets only have a meaning for a specific partition. the offsets are repeated across partitions.
  - the data when sent to a Kafka topic, is going to be assigned to a random partition, Unless you provide a message key,

# Producer and Message key
  - Producers write data to Kafka topics and partitions.
  - Topics have partitions, and writes happen sequentially with data offsets.
  - Producers send data to Kafka topic partitions.
  - Producers know in advance to which partition they write.
  - Producers send data across all partitions using load balancing.
  - Kafka scales due to multiple partitions within a topic.
  - Producers assign keys to messages for ordering and identification.
  - Messages with the same key go to the same partition.
  - Producers decide where messages end up based on the key.
  - Kafka messages are created using serializers to transform objects into bytes.
  - Key and value serializers can be different.
  - Kafka partitioners determine which partition a message goes to.
  - Key hashing uses algorithms like murmur2 for mapping keys to partitions.

![image](https://github.com/SbrTa/Notes/assets/8649145/e513078f-ef87-4185-a792-9f697537dae0)


# Consumer and Deserialization
- Consumers are used to read data from Kafka topics.
- They operate on a pull model, requesting data from Kafka brokers.
- Consumers can choose the partitions they read from.
- An example with three partitions demonstrates different consumers reading from specific partitions.
- Consumers automatically handle broker failures and know how to recover.
- Data is read in order within each partition, from low to high offset.
- Consumers need to transform bytes received from Kafka into usable objects.
- Deserializers are used to convert binary key and value formats into appropriate objects.
- Consumers must know the format of messages in advance.
- Deserializers, bundled with Kafka, handle the transformation based on expected formats (e.g., integer, string).
- Changing the data type of a topic after creation can break consumers.
- Creating a new topic with the desired format is recommended for changes, requiring consumer adjustments.
- Consumers play a vital role in reading and processing Kafka messages.
- Understanding serialization and data format changes is crucial for effective consumer usage.

# Consumers Groups & consumer offset
- Kafka applications scale by having multiple consumers in a group.
- Consumers within a group read data collectively from Kafka topics.
- Each consumer reads from exclusive partitions, ensuring efficient distribution.
- If there are more consumers than partitions, additional consumers may remain inactive.
- Inactive consumers won't read from any partitions.
- It's acceptable to have multiple consumer groups on the same topic.
- Each partition may have multiple readers, but within a consumer group, only one consumer is assigned to one partition.
- Distinct consumer groups are created using the `group.id` consumer property.
- Multiple consumer groups are useful for different services, like a location service and a notification service.
- Kafka stores consumer offsets in an internal topic named "consumers_offsets."
- Committing offsets is crucial for tracking the progress of a consumer group.
- Consumer offsets allow replaying data in case of consumer failures or restarts.
- Consumers can commit offsets either automatically (at least once by default) or manually.
- Three delivery semantics: at least once, at most once, and exactly once.
- Considerations for idempotent processing to avoid duplicate impacts.


# Broker & Topics

A Kafka cluster is an ensemble of multiple Kafka brokers. Unlike typical servers, Kafka brokers are specialized for receiving and sending data.

Each Kafka broker is identified by an integer ID, such as Broker 101, Broker 102, and Broker 103 in our cluster.


Brokers contain specific topic partitions, leading to distributed data across all brokers in the cluster. This distribution ensures scalability, and Kafka can handle clusters with varying numbers of brokers, ranging from a few to over 100.

In our example, we've discussed numbering brokers starting at 100 for simplicity.


Let's consider an example with Topic-A having three partitions and Topic-B having two partitions. We have three brokers: Broker 101, Broker 102, and Broker 103.

- Broker 101 has Topic-A, Partition 0.
- Broker 102 has Topic-A, Partition 2.
- Broker 103 has Topic-A, Partition 1.

For Topic-B:
- Broker 101 has Topic-B, Partition 1.
- Broker 102 has Topic-B, Partition 0.

This distribution illustrates Kafka's horizontal scaling, where data and partitions are spread across the entire cluster.


Kafka uses a bootstrap server mechanism for broker discovery. When connecting to any Kafka broker (bootstrap broker), clients automatically learn how to connect to the entire cluster. This smart mechanism simplifies cluster connection for clients.

# Topic Replication

When you're working on your own machine, topics can have a replication factor of one. However, in production, especially with a real Kafka cluster, the replication factor is usually set greater than one, commonly between two and three. This ensures that if a broker is down for maintenance or technical issues, other brokers still have copies of the data to serve and receive.

Let's take an example to understand this better. We have Topic-A with two partitions and a replication factor of two. Three Kafka brokers are in play.
- Partition 0 of Topic-A is on broker 101.
- Partition 1 of Topic-A is on broker 102.

Due to the replication factor, we have copies of partition 0 on broker 102 and partition 1 on broker 103. In case we lose broker 102, broker 101 and 103 can still serve the data, ensuring availability.

Replicas and Leaders:
- Kafka operates with a leader for each partition.
- Producers can only send data to the leader broker of a partition.
- Consumers read data primarily from the leader of a partition.

This design ensures a reliable and efficient data flow within Kafka clusters. 

Additionally, Kafka Consumer Replica Fetching, introduced in Kafka 2.4, allows consumers to read from the closest replica, improving latency and potentially reducing network costs.
