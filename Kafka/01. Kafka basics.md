# Topics, Partitions and Offset
  - Kafka topics are a particular stream of data within your Kafka cluster.
  - Kafka cluster can have many topics.
  - A topic is identified by its name.
  - So, a topic in Kafka is a stream of data.
  - Kafka is called a data streaming platform, because you make data stream through topics.
  - A topic is similar to what a table would be in a database, but without all the constraints. Because you send whatever you want to a Kafka topic, there is no data verification.
  - you can send, for example, Json, Avro, text file, binary, whatever you want.
  - So topics are similar to table database, but you cannot query them, instead to add data into a Kafka topic, And read data from a topic.
  - you can divide topics into partitions. a topic can be made up of for example, 100 partitions.
  - Now the messages sent to Kafka topic are going to end up in these partitions,
  - messages within each partition are going to be ordered.
  - So, the messages in these partitions where they are written, they are getting a id, that's incrementing from zero to whatever. And this id is called a Kafka partition offset.
  - Kafka topics are also immutable. That means that once the data is written into a partition, it cannot be changed.
  - Data in Kafka is only kept for a limited time. And the default is one week, although that is configurable. That means that after one week, your data will disappear.
  - And the offsets only have a meaning for a specific partition. the offsets are repeated across partitions.
  - the data when sent to a Kafka topic, is going to be assigned to a random partition, Unless you provide a message key,

# Producer and Message key
  - Producers write data to Kafka topics and partitions.
  - Topics have partitions, and writes happen sequentially with data offsets.
  - Producers send data to Kafka topic partitions.
  - Producers know in advance to which partition they write.
  - Producers send data across all partitions using load balancing.
  - Kafka scales due to multiple partitions within a topic.
  - Producers assign keys to messages for ordering and identification.
  - Messages with the same key go to the same partition.
  - Producers decide where messages end up based on the key.
  - Kafka messages are created using serializers to transform objects into bytes.
  - Key and value serializers can be different.
  - Kafka partitioners determine which partition a message goes to.
  - Key hashing uses algorithms like murmur2 for mapping keys to partitions.

![image](https://github.com/SbrTa/Notes/assets/8649145/e513078f-ef87-4185-a792-9f697537dae0)


# Consumer and Deserialization
- Consumers are used to read data from Kafka topics.
- They operate on a pull model, requesting data from Kafka brokers.
- Consumers can choose the partitions they read from.
- An example with three partitions demonstrates different consumers reading from specific partitions.
- Consumers automatically handle broker failures and know how to recover.
- Data is read in order within each partition, from low to high offset.
- Consumers need to transform bytes received from Kafka into usable objects.
- Deserializers are used to convert binary key and value formats into appropriate objects.
- Consumers must know the format of messages in advance.
- Deserializers, bundled with Kafka, handle the transformation based on expected formats (e.g., integer, string).
- Changing the data type of a topic after creation can break consumers.
- Creating a new topic with the desired format is recommended for changes, requiring consumer adjustments.
- Consumers play a vital role in reading and processing Kafka messages.
- Understanding serialization and data format changes is crucial for effective consumer usage.

