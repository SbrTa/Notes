# Latency and Throughput

### Latency
  - Latency refers to the amount of time it takes for data to be sent to its intended destination. 
  - It’s also referred to as the delay in moving data between two clients.
  - You may say that a second delay is no big deal, but that is not always the case. 
  - Let’s take a look at a real-world scenario of how latency can affect your network and browsing experience. 
    - For example, let’s see how latency affects online gaming.
    - With excellent latency (a low number) your actions in-game will be almost instant compared to your inputs.
    - With high latency, however, there might be a long delay between your clicks and what happens on the screen.
    - That might be fine if you are watching a movie or browsing the web. 
    - But for real-time interactions, low latency is a need. 
    - Otherwise, the experience is negatively affected.
  - The time it takes for a certain operation to complete in a system. Most often this measure is a time duration, like milliseconds or seconds. You should know these orders of magnitude:
    - Reading 1 MB from RAM: 250 μs (0.25 ms)
    - Reading 1 MB from SSD: 1,000 μs (1 ms)
    - Transfer 1 MB over Network: 10,000 μs (10 ms)
    - Reading 1MB from HDD: 20,000 μs (20 ms)
    - Inter-Continental Round Trip: 150,000 μs (150 ms)

### Throughput
  - Throughput is the amount of data that is successfully transmitted through a system in a certain amount of time, measured in bits per second (bps). 
  - The number of operations that a system can handle properly per time unit. For instance the throughput of a server can often be measured in requests per second (RPS or QPS).


### Throughput and Latency
  - The combination of throughput and latency is the best network performance metric.
  - By measuring both, you are able to know the amount of data that is being sent over a specific amount of time. 
  - They also need to work together if we want to achieve good results. 
  - It’s no good to have good throughput if latency is through the roof.
  - Yes, you’ll send a lot of data, but it will take too long to reach its destination. And even longer to get a reply.
  - Conversely, having ultra-low latency is no good if you can only send small chunks of data at a time. It will still take too long for all the information to travel to its destination.
